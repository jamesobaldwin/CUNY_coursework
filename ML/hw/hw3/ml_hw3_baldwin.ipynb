{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88c3ac2b-6da3-411f-a79d-ba71e3a7df78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import pinv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a442b77-647a-4c87-8ca0-55ca5c87a4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grad the dataset from sklearn datasets\n",
    "in_data = datasets.load_breast_cancer()\n",
    "# store features and data points (569 data points X 30 features)\n",
    "df = pd.DataFrame(in_data.data, columns=in_data.feature_names)\n",
    "# store data in array\n",
    "data = df.values\n",
    "# store binary diagnoses for 569 data points (0=malignant, 1=benign)\n",
    "targets = in_data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05825666-2c11-46b4-b036-88cf7d8738fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# helper functions\n",
    "###################\n",
    "\n",
    "def init_weights(data, targets):\n",
    "    X = data\n",
    "    y = targets\n",
    "    # add bias term x0=1 to the data\n",
    "    X = np.column_stack((np.ones(X.shape[0]), X))\n",
    "    # initialize weights using linear regression\n",
    "    X_dag = pinv(X)\n",
    "    w = np.matmul(X_dag, y)\n",
    "\n",
    "    expected_shape = (X.shape[1],)\n",
    "\n",
    "    # Check the shape of the result\n",
    "    if w.shape != expected_shape:\n",
    "        raise ValueError(f\"Output has shape {result.shape}, but expected shape is {expected_shape}.\")\n",
    "    \n",
    "    return X, w\n",
    "\n",
    "def sigmoid(x):\n",
    "  return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def poly_transform(X, degree: int=2):\n",
    "    # Create the PolynomialFeatures transformer\n",
    "    poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    \n",
    "    # Transform the data\n",
    "    X_poly = poly.fit_transform(X)\n",
    "    \n",
    "    # Print the shape of the transformed feature space\n",
    "    print(X_poly.shape)  # Output will be (100, 495)\n",
    "\n",
    "    return X_poly\n",
    "\n",
    "def regularization():\n",
    "\n",
    "def separate_test(data, targets, num_test_elems: int=69, random_state: int=None):\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "        \n",
    "    X = data\n",
    "    num_remove = num_test_elems\n",
    "    # store indices for removal from data and use as test set\n",
    "    remove_indices = np.random.choice(np.arange(X.shape[0]), size=num_remove, replace=False)\n",
    "    X_test = X[remove_indices, :]    # shape (69,20)\n",
    "    y_test = targets[remove_indices]   # shape (69,)\n",
    "    \n",
    "    # remove the test set from the training data\n",
    "    X = np.delete(X, remove_indices, axis=0)   # shape (500, 20)\n",
    "    y = np.delete(targets, remove_indices, axis=0)   # shape (500,)\n",
    "    \n",
    "    return X, X_test, y, y_test\n",
    "\n",
    "def create_train_val(X, y, N_arr, random_state: int=None):\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "    # split the training data into 10 sample sets with training and validation sets\n",
    "    X_train_set = []\n",
    "    y_train_set = []\n",
    "    for num in N_arr:\n",
    "        if num < 500:\n",
    "            X_train, _, y_train, _ = train_test_split(X, y, train_size=num, random_state=42)\n",
    "        else: \n",
    "            X_train = X; y_train = y\n",
    "            \n",
    "        X_train_set.append(X_train); y_train_set.append(y_train);\n",
    "\n",
    "    return X_train_set, y_train_set\n",
    "\n",
    "#########################\n",
    "# model helper functions\n",
    "#########################\n",
    "def predict(X, w):\n",
    "    return np.sign(np.dot(X,w))\n",
    "\n",
    "def calculate_Ein(X, y, w):\n",
    "    predictions = predict(X, w)\n",
    "    return np.sum(predictions != y) / len(y)\n",
    "\n",
    "def plot_results(Ein, Eout, N, title, filename):\n",
    "\n",
    "    plt.plot(N, Ein*100, label='Ein', marker='o')\n",
    "    plt.plot(N, Eout*100, label='Eout', marker='o')\n",
    "    plt.xlabel('Size of Training Data Set')\n",
    "    plt.ylabel('Error (%)')\n",
    "    plt.title(title)\n",
    "    plt.xticks(N, label=N)\n",
    "    plt.tick_params(axis='both', direction='in')\n",
    "    plt.grid(alpha=0.4)\n",
    "    plt.legend()\n",
    "    plt.savefig(filename)\n",
    "    plt.plot()\n",
    "\n",
    "###################\n",
    "# pocket algorithm\n",
    "###################\n",
    "def pocket(X, y, w, max_iters: int = 1000, random_state: int=None):\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "    '''\n",
    "    Vectorized version of the pocket algorithm \n",
    "    based on the algorithm outlined in Learning from Data S3.1, pg 80.\n",
    "    \n",
    "    Args:\n",
    "        X: set of input data points (N x d array)\n",
    "        y: true value of output (N, array)\n",
    "        w: initial weights (d, array)\n",
    "        max_iters: maximum number of iterations before exiting\n",
    "\n",
    "    Returns:\n",
    "        pocket_w: pocket weights associated with the model\n",
    "        min_Ein: the minimum error of the pocket weights\n",
    "    '''\n",
    "    N, d = X.shape  # N: number of points, d: number of features\n",
    "    pocket_w = w.copy()  # Set pocket weight vector to initial weights\n",
    "    Ein_min = calculate_Ein(X, y, w)  # Initialize pocket error as current error\n",
    "\n",
    "    for t in range(max_iters):\n",
    "        # Vectorized prediction\n",
    "        predictions = np.sign(X @ w)\n",
    "        # Find misclassified points\n",
    "        misclassified = np.where(predictions != y)[0]\n",
    "\n",
    "        # Stop if no misclassified points (PLA converged)\n",
    "        if misclassified.size == 0:\n",
    "            print(f'PLA converged after {t} iterations.')\n",
    "            break\n",
    "\n",
    "        # Randomly pick a misclassified point and update weights\n",
    "        random_idx = np.random.choice(misclassified)\n",
    "        w += y[random_idx] * X[random_idx]  # PLA update\n",
    "\n",
    "        # Evaluate Ein for the new weight vector w(t+1)\n",
    "        current_Ein = calculate_Ein(X, y, w)\n",
    "\n",
    "        # If w(t+1) is better than pocket_w, update pocket_w\n",
    "        if current_Ein < Ein_min:\n",
    "            pocket_w = w.copy()  # Update pocket weights\n",
    "            Ein_min = current_Ein  # Update minimum Ein\n",
    "\n",
    "    # Return the best pocket weights\n",
    "    return pocket_w, Ein_min\n",
    "\n",
    "################\n",
    "# evaluation \n",
    "################\n",
    "\n",
    "def calculate_Ein_vals(X, y, w, N: int=10):\n",
    "    best_weights = []\n",
    "    E_ins = []\n",
    "    for i in range(N):\n",
    "        best_weight, Ein = pocket(X[i], y[i], w)\n",
    "        best_weights.append(best_weight)\n",
    "        E_ins.append(Ein)\n",
    "\n",
    "    return np.array(E_ins), np.array(best_weights)\n",
    "\n",
    "def calculate_error(X_set, y_set, best_weights):\n",
    "    \"\"\"\n",
    "    Calculate errors and number of misclassified points for each subset.\n",
    "    \n",
    "    Args:\n",
    "        X_val: List or array-like of shape (10, N, n) containing validation data subsets.\n",
    "        y_val: List or array-like of shape (10, N) containing true labels for each subset.\n",
    "        best_weights: List of weight arrays, one for each subset.\n",
    "        \n",
    "    Returns:\n",
    "        num_misclassified: NumPy array of number of misclassified points for each subset.\n",
    "        val_errors: NumPy array of validation error rates for each subset.\n",
    "    \"\"\"\n",
    "    num_misclassified = []\n",
    "    errors = []\n",
    "    \n",
    "    # Iterate over each subset of validation data\n",
    "    for i in range(len(X_set)):\n",
    "        predictions = predict(X_set[i], best_weights[i])\n",
    "        missed = np.sum(predictions != y_set[i])  # Count how many predictions are incorrect\n",
    "        error = missed / len(y_set[i])  # Calculate the error rate\n",
    "        \n",
    "        # Store the results for each subset\n",
    "        num_misclassified.append(missed)\n",
    "        errors.append(error)\n",
    "    \n",
    "    return np.array(num_misclassified), np.array(errors)\n",
    "\n",
    "def calculate_Eouts(X_test, y_test, best_weights):\n",
    "    num_misclassified = []\n",
    "    test_errors = []\n",
    "    for i in range(len(best_weights)):\n",
    "        predictions = predict(X_test, best_weights[i])\n",
    "        missed = np.sum(predictions != y_test)\n",
    "        test_error = missed / len(y_test)\n",
    "\n",
    "        num_misclassified.append(missed)\n",
    "        test_errors.append(test_error)\n",
    "\n",
    "    return np.array(missed), np.array(test_errors)\n",
    "\n",
    "\n",
    "############################\n",
    "# function to run the model\n",
    "# and output results\n",
    "############################\n",
    "\n",
    "def run_model(data, targets, title, filename, random_state: int=42):\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    ## array of data set sizes\n",
    "    N = np.array([10,50,100,200,250,300,350,400,450,500])\n",
    "    \n",
    "    ####################\n",
    "    #  prep data\n",
    "    ####################\n",
    "    # initialize the weights using linear regression\n",
    "    X_bias, w = init_weights(data, targets)\n",
    "    # separate out the test data\n",
    "    X, X_test, y, y_test = separate_test(X_bias, targets)\n",
    "    # create training and validation sets\n",
    "    X_train_set, y_train_set = create_train_val(X, y, N)\n",
    "\n",
    "    #########################\n",
    "    # calculate Ein and Eout\n",
    "    #########################\n",
    "    Eins, best_weights = calculate_Ein_vals(X_train_set, y_train_set, w)\n",
    "    num_misses, Eouts = calculate_Eouts(X_test, y_test, best_weights)\n",
    "    print('N: Ein   Eout')\n",
    "    for i, n in enumerate(N):\n",
    "        print(f'{n}: {100*Eins[i]:.02f}%  {100*Eouts[i]:.02f}%')\n",
    "\n",
    "    #########################\n",
    "    # plot results\n",
    "    #########################\n",
    "    plot_results(Eins, Eouts, N, title, filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
